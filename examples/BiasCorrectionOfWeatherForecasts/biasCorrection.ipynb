{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-correction for weather forecasts\n",
    "\n",
    "We will use IBM PAIRS to study and correct biases in the GFS forecast. In detail, we will compare the temperature forecast made by NOAA's GFS with the ERA5 reanalysis. Aggregating 5 years worth of data will give evidence of a systematic yet location-dependent bias in the GFS data. We will then use PAIRS to remove these biases from the GFS forecast, leading to an MAE reduction of roughly 50%.\n",
    "\n",
    "For details regarding both data layers, consult the PAIRS data explorer at https://ibmpairs.mybluemix.net/data-explorer. The layer IDs are\n",
    "- 49423 (ERA5)\n",
    "- 50195 (GFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os, numpy as np, pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from ibmpairs import paw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we set some global variables. I.e. the name of your user account et cetera. Note that this assumes the existence of a file `ibmpairspass.txt` in your home directory containing access credentials for IBM PAIRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAIRS_USER              = '<username>'\n",
    "PAIRS_SERVER            = 'https://pairs.res.ibm.com'\n",
    "BASE_URI                = '/'\n",
    "PAIRS_PASSWORD          = paw.get_pairs_api_password(PAIRS_SERVER, PAIRS_USER, passFile=os.path.expanduser('~/ibmpairspass.txt'))\n",
    "PAIRS_CREDENTIALS       = (PAIRS_USER, PAIRS_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following helps when converting `datetime` objects to strings in ISO 8601-compliant format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso8601 = '%Y-%m-%dT%H:%M:%SZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Temporal joins between the GFS and ERA5 data\n",
    "\n",
    "Since the GFS data is a forecast, it does not only depend on a single timestamp. Indeed, a forecast is characterized both by its *valid time* and *issue time*. I.e. the time the forecast is for and the time it was issued. The difference between these is known as the *lead time* or *horizon*. I.e. `horizon = valid time - issue time`. Most forecasts in PAIRS are stored by issue time, simplifying the comparison with observations. The horizon is then an additional dimension, usually in hours. (For details consult the data explorer.)\n",
    "\n",
    "To keep things simple, we will only consider the GFS data at horizon 6 h. I.e. predictions which were made 6 hours into the future. (Since the GFS predicts up to 15 days ahead, the maximum horizon is 360 hours.) The forecast is issued daily, which means that as long as we keep the horizon fixed at 6 we have one value every 24 hours. (To obtain data at higher frequencies, we could query additional horizons.)\n",
    "\n",
    "The ERA5 data on the other hand is hourly. To join these two, we have to ensure that we only request timestamps for which both are defined. (The situation would be a bit different for a parameter which is accumulcated over a certain amount of time such as precipitation. Since temperature is generally considered an instantaneous quantity, we can simply join identical timestamps. Details on whether a parameter is instantaneous or defined over a certain measurement interval can be found in the data explorer.\n",
    "\n",
    "Thus, we make a point query to both data layers to find all timestamps during the years 2014 to 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointQueryJson = {\n",
    "    'layers' : [\n",
    "        {\n",
    "            'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}]\n",
    "        },\n",
    "        {\n",
    "            'type' : 'raster', 'id' : '49423'\n",
    "        }\n",
    "    ],\n",
    "    'spatial' : {'type' : 'point', 'coordinates' : ['40', '-100']},\n",
    "    'temporal' : {'intervals' : [{\n",
    "        'start' : (datetime(2014, 1, 1) - timedelta(seconds = 1)).strftime(iso8601),\n",
    "        'end' : datetime(2019, 1, 1).strftime(iso8601)\n",
    "    }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointQuery = paw.PAIRSQuery(pointQueryJson, PAIRS_SERVER, PAIRS_CREDENTIALS)\n",
    "pointQuery.submit()\n",
    "pointQuery.vdf['value'] = pd.to_numeric(pointQuery.vdf['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple *pandas* transformations allow us to identify timestamps for which both layers are defined.\n",
    "\n",
    "**Note** we can use this simple point query since both the forecast and reanalysis data is defined for all locations in the world at the same time. I.e. if we find data at a single time and point, we know that there will be data everywhere else in the world at that timestamp. This would not be the case were we to compare satellite data from e.g. Sentinel or Landsat satellites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointQuery.vdf.pivot_table(index = 'timestamp', columns = 'layerId', values = 'value').dropna().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDates = pointQuery.vdf.pivot_table(index = 'timestamp', columns = 'layerId', values = 'value').dropna().index.to_series(keep_tz = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Identifying the overall bias\n",
    "\n",
    "To proceed, we use the timestamps stored in `completeDates` to compare the GFS and ERA5 data at those timestamps. Since taking the mean and subtraction are interchangable operations, we take the mean for each dataset before calculating the bias. (Simply taking the average of all values in 2014-2018 would have led to wildly inconsistent results. At fixed horizon, the GFS layer only contains values for 0:00 UTC while the ERA5 one contains hourly data. Thus we would have compared aggregates over 0:00 UTC with those over all hours of the day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasQueryJson = {\n",
    "    'layers' : [\n",
    "        {\n",
    "            'alias' : 'gfs',\n",
    "            'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}],\n",
    "            'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'era5',\n",
    "            'type' : 'raster', 'id' : '49423', 'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'bias',\n",
    "            'expression' : '$gfs - $era5'\n",
    "        }\n",
    "    ],\n",
    "    'spatial' : {'type' : 'square', 'coordinates' : ['-90', '-170', '90', '170']},\n",
    "    'temporal' : {'intervals' : [{\n",
    "        'start' : (datetime(2015, 1, 1) - timedelta(seconds = 1)).strftime(iso8601),\n",
    "        'end' : datetime(2016, 1, 1).strftime(iso8601)\n",
    "    }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasQuery = paw.PAIRSQuery(biasQueryJson, PAIRS_SERVER, PAIRS_CREDENTIALS)\n",
    "biasQuery.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The queries in this notebook are somewhat substantial and should take 5-10 minutes to complete. Calling `.poll_till_finished()` will thus block the notebook for an extended time. It's worth while to check the status of the query by calling `.poll()` and checking the `.queryStatus` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasQuery.poll()\n",
    "biasQuery.queryStatus.json()['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasQuery.poll_till_finished()\n",
    "biasQuery.download()\n",
    "biasQuery.create_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, we find fairly clear evidence of systematic bias. Temperatures in North- and South-America are generally predicted as too low while those in North and South-Africa appear to be too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 8))\n",
    "plt.imshow(\n",
    "    biasQuery.data['Expression-bias[bias]-Exp'],\n",
    "    vmin = -5, vmax = 5, cmap = 'seismic',\n",
    "    extent = [biasQuery.metadata['Expression-bias[bias]-Exp']['details']['boundingBox'][l] for l in ['minLongitude', 'maxLongitude', 'minLatitude', 'maxLatitude']]\n",
    ")\n",
    "plt.colorbar(label = 'Error [K]')\n",
    "plt.title('Mean bias in GFS data 2014-2018')\n",
    "plt.savefig('MeanBiasInGFS2014-2018.png', dpi = 60, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having said that, it is instructive to look at the overall error distribution. Accumulating spatially leads to what is an essentially neglegible bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(biasQuery.data['Expression-bias[bias]-Exp'].reshape(-1)).dropna().hist(bins = 25, log = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(biasQuery.data['Expression-bias[bias]-Exp'].reshape(-1)).dropna().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we note an mean absolute error (MAE) of about 0.86 degrees Kelvin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(biasQuery.data['Expression-bias[bias]-Exp'].reshape(-1)).dropna().abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Temporal dependence of the biases\n",
    "\n",
    "One might ask whether the bias we identified in the previous query is a systematic bias or an artifact of the aggregation. So let us take a further look. The following query calculates the bias for each year in the 5-year period independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualLayers = [\n",
    "    [\n",
    "    {\n",
    "        'alias' : 'gfs_{}'.format(year),\n",
    "        'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}],\n",
    "        'aggregation' : 'Mean',\n",
    "        'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems() if ts.year == year]},\n",
    "        'output' : False\n",
    "    },\n",
    "    {\n",
    "        'alias' : 'era5_{}'.format(year),\n",
    "        'type' : 'raster', 'id' : '49423', 'aggregation' : 'Mean',\n",
    "        'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems() if ts.year == year]},\n",
    "        'output' : False\n",
    "    },\n",
    "    {\n",
    "        'alias' : 'bias_{}'.format(year),\n",
    "        'expression' : '$gfs_{year} - $era5_{year}'.format(year = year),\n",
    "    }\n",
    "    ] for year in range(2014, 2019)\n",
    "]\n",
    "annualLayers = [ll for l in annualLayers for ll in l]\n",
    "\n",
    "annualBiasQueryJson = {\n",
    "    'layers' : annualLayers,\n",
    "    'spatial' : {'type' : 'square', 'coordinates' : ['-90', '-170', '90', '170']},\n",
    "    'temporal' : {'intervals' : [{\n",
    "        'start' : (datetime(2015, 1, 1) - timedelta(seconds = 1)).strftime(iso8601),\n",
    "        'end' : datetime(2016, 1, 1).strftime(iso8601)\n",
    "    }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualBiasQuery = paw.PAIRSQuery(annualBiasQueryJson, PAIRS_SERVER, PAIRS_CREDENTIALS)\n",
    "annualBiasQuery.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualBiasQuery.poll()\n",
    "annualBiasQuery.queryStatus.json()['status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annualBiasQuery.poll_till_finished()\n",
    "annualBiasQuery.download()\n",
    "annualBiasQuery.create_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the plot simple we only show data for 2014 - 2017. The situation is essentially the same for 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = extent = [annualBiasQuery.metadata['Expression-bias_2014[bias_2014]-Exp']['details']['boundingBox'][l] for l in ['minLongitude', 'maxLongitude', 'minLatitude', 'maxLatitude']]\n",
    "fig, ax = plt.subplots(2, 2, figsize = (30, 16), sharex = True, sharey = True)\n",
    "ax[0][0].imshow(annualBiasQuery.data['Expression-bias_2014[bias_2014]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent)\n",
    "ax[0][1].imshow(annualBiasQuery.data['Expression-bias_2015[bias_2015]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent)\n",
    "ax[1][0].imshow(annualBiasQuery.data['Expression-bias_2016[bias_2016]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent)\n",
    "ax[1][1].imshow(annualBiasQuery.data['Expression-bias_2017[bias_2017]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent)\n",
    "ax[0][0].set_title('2014')\n",
    "ax[0][1].set_title('2015')\n",
    "ax[1][0].set_title('2016')\n",
    "ax[1][1].set_title('2017')\n",
    "plt.tight_layout()\n",
    "plt.savefig('TemporalDependenceOfBias.png', dpi = 60, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the biases are fairly consistent with time. We see differences in the details between the above aggregation periods, but overall structures are the same. I.e. too low predictions in the Americas, Europe and at the North Pole, too high predictions in North and South Africa, Northern and Central Asia as well as Antarctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: A bias corrected forecast\n",
    "\n",
    "Having confirmed the existence of a bias, we can now issue a bias-corrected forecast for 2019. I.e. we use the bias measured during the previous 5-year period to shift the predictions for 2019. To start, we again make a point query to facilitat the temporal join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointQuery2019Json = {\n",
    "    'layers' : [\n",
    "        {\n",
    "            'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}]\n",
    "        },\n",
    "        {\n",
    "            'type' : 'raster', 'id' : '49423'\n",
    "        }\n",
    "    ],\n",
    "    'spatial' : {'type' : 'point', 'coordinates' : ['40', '-100']},\n",
    "    'temporal' : {'intervals' : [{\n",
    "        'start' : (datetime(2019, 1, 1) - timedelta(seconds = 1)).strftime(iso8601),\n",
    "        'end' : datetime(2019, 7, 1).strftime(iso8601)\n",
    "    }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointQuery2019 = paw.PAIRSQuery(pointQuery2019Json, PAIRS_SERVER, PAIRS_CREDENTIALS)\n",
    "pointQuery2019.submit()\n",
    "pointQuery2019.vdf['value'] = pd.to_numeric(pointQuery2019.vdf['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDates2019 = pointQuery2019.vdf.pivot_table(index = 'timestamp', columns = 'layerId', values = 'value').dropna().index.to_series(keep_tz = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query again calculates the forecast bias of the years 2014-2018 and applies it to forecasts for the first half of 2019. Subsequently we calculate the bias of both the raw and bias-corrected 2019 forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasCorrectedQueryJson = {\n",
    "    'layers' : [\n",
    "        {\n",
    "            'alias' : 'historic_gfs',\n",
    "            'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}],\n",
    "            'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'historic_era5',\n",
    "            'type' : 'raster', 'id' : '49423', 'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'gfs',\n",
    "            'type' : 'raster', 'id' : '50195', 'dimensions' : [{'name' : 'horizon', 'value' : '6'}],\n",
    "            'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates2019.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'era5',\n",
    "            'type' : 'raster', 'id' : '49423', 'aggregation' : 'Mean',\n",
    "            'temporal' : {'intervals' : [{'snapshot' : ts.strftime(iso8601)} for _, ts in completeDates2019.iteritems()]},\n",
    "            'output' : False\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'bias',\n",
    "            'expression' : '$gfs - $era5',\n",
    "            'output' : True\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'bias_corrected_forecast',\n",
    "            'expression' : '$gfs - $historic_gfs + $historic_era5',\n",
    "            'output' : True\n",
    "        },\n",
    "        {\n",
    "            'alias' : 'bias_of_bias_corrected_forecast',\n",
    "            'expression' : '$gfs - $historic_gfs + $historic_era5 - $era5',\n",
    "            'output' : True\n",
    "        }\n",
    "    ],\n",
    "    'spatial' : {'type' : 'square', 'coordinates' : ['-90', '-170', '90', '170']},\n",
    "    'temporal' : {'intervals' : [{\n",
    "        'start' : (datetime(2015, 1, 1) - timedelta(seconds = 1)).strftime(iso8601),\n",
    "        'end' : datetime(2016, 1, 1).strftime(iso8601)\n",
    "    }]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasCorrectedQuery = paw.PAIRSQuery(biasCorrectedQueryJson, PAIRS_SERVER, PAIRS_CREDENTIALS)\n",
    "biasCorrectedQuery.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasCorrectedQuery.poll_till_finished()\n",
    "biasCorrectedQuery.download()\n",
    "biasCorrectedQuery.create_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Results\n",
    "\n",
    "We can finally analyze the impact of the bias correction. To do so we both plot the spatial distribution of biases but also calculate a number of global metrics. To start, let's take a look at the spatial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = extent = [biasCorrectedQuery.metadata['Expression-bias[bias]-Exp']['details']['boundingBox'][l] for l in ['minLongitude', 'maxLongitude', 'minLatitude', 'maxLatitude']]\n",
    "fig, ax = plt.subplots(2, 1, figsize = (30, 16), sharex = True, sharey = True)\n",
    "ax[0].imshow(\n",
    "    biasCorrectedQuery.data['Expression-bias[bias]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent\n",
    ")\n",
    "ax[1].imshow(\n",
    "    biasCorrectedQuery.data['Expression-bias_of_bias_corrected_forecast[bias_of_bias_corrected_forecast]-Exp'], vmin = -5, vmax = 5, cmap = 'seismic', extent = extent\n",
    ")\n",
    "ax[0].set_title('Bias of raw forecast')\n",
    "ax[1].set_title('Bias of bias-corrected forecast')\n",
    "plt.tight_layout()\n",
    "plt.savefig('PerformanceOfBiasCorrectedForecast.png', dpi = 60, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the two plots above are on the same color scale (see the use of `vmin` and `vmax`), we can make direct comparisons from the colors. Clearly, the bias corrected forecast shows an overall improvement. Note that the underprediction in the Americas, the Indian subcontinent and Europe has turned into an overprediction. The situation is somewhat more complicated in the rest of the world though.\n",
    "\n",
    "To complement this impression we calculate some general metrics. Note that the mean absolute error (MAE) has improved quite significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Raw forecast' : pd.Series(biasCorrectedQuery.data['Expression-bias[bias]-Exp'].reshape(-1)).dropna().describe().append(pd.Series({'MAE' : pd.Series(biasCorrectedQuery.data['Expression-bias[bias]-Exp'].reshape(-1)).dropna().abs().mean()})),\n",
    "    'Bias-corrected forecast' : pd.Series(biasCorrectedQuery.data['Expression-bias_of_bias_corrected_forecast[bias_of_bias_corrected_forecast]-Exp'].reshape(-1)).dropna().describe().append(pd.Series({'MAE' : pd.Series(biasCorrectedQuery.data['Expression-bias_of_bias_corrected_forecast[bias_of_bias_corrected_forecast]-Exp'].reshape(-1)).dropna().abs().mean()}))\n",
    "}).round(2)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "136.4px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
