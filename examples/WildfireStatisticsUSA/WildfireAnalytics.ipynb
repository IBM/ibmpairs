{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire statistics\n",
    "\n",
    "PAIRS layer `50035` contains information on _burned areas_. I.e. a pixel has the value 1 on the timestamp a burn occured. (The data is derived from http://modis-fire.umd.edu/ba.html.) Leveraging the spatial and temporal aggregation features in PAIRS allows us to calculate burned areas for each state in the US and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, logging, matplotlib.pyplot as plt, re, requests, sklearn\n",
    "from datetime import datetime, timedelta\n",
    "from ibmpairs import paw\n",
    "\n",
    "os.environ['PAW_PAIRS_DEFAULT_PASSWORD_FILE_NAME'] = '<path to ibmpairspass.txt>'\n",
    "os.environ['PAW_PAIRS_DEFAULT_USER'] = '<username>'\n",
    "os.environ['PAW_LOG_LEVEL_ENV'] = 'WARNING'\n",
    "paw.load_environment_variables()\n",
    "\n",
    "logging.basicConfig(level = logging.WARNING)\n",
    "pawLogger = logging.getLogger('ibmpairs.paw')\n",
    "pawLogger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso8601 = '%Y-%m-%dT%H:%M:%SZ'\n",
    "yearStart, yearEnd = 2001, 2019\n",
    "burnableLand = [62, 63, 64, 141, 142, 143, 152, 176]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following JSON defines the PAIRS query. We are requesting years 2010 to 2018. Since we are interested in the aggregate annual burned areas, we are performing a `Max` aggregation. (Since pixels in this layer have the value 1 exclusively, `Min` and `Mean` would give the same result. `Sum` would not for pixels that were affected by multiple fires.)\n",
    "\n",
    "Note that spatial aggregation is always performed _last_ in PAIRS. In the case at hand, we aggregate across US states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryJson = {\n",
    "    \"layers\": [\n",
    "        {   \n",
    "            \"alias\" : \"B{y}\".format(y = y),\n",
    "            \"id\": '50035',\n",
    "            \"output\": True,\n",
    "            \"temporal\" : {\"intervals\" : [{\n",
    "                \"start\" : datetime(y, 1, 1).strftime(iso8601), \"end\" : datetime(y+1, 1, 1).strftime(iso8601)\n",
    "            }]},\n",
    "            \"aggregation\" : \"Max\",\n",
    "            \"type\": \"raster\"\n",
    "        }\n",
    "    for y in range(yearStart, yearEnd)],\n",
    "    \"name\": \"Burned areas\",\n",
    "    \"spatial\" : {\n",
    "        \"type\" : \"poly\", \"aoi\" : \"24\",\n",
    "        \"aggregation\": {\n",
    "            \"aoi\": [\n",
    "                i for i in range(121, 172)\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"temporal\": {\"intervals\": [{\"start\" : datetime(2019, 1, 1).strftime(iso8601), \"end\" : datetime(2020, 1, 1).strftime(iso8601)}]},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are not only interested in the wildfire data. Since PAIRS offers a multitude of additional data layers, we can look for statistically significant dependencies. Thus we add layers `92` and `49069`. The former is the daily maximum temperature as reported by PRISM, the latter daily averaged soil moisture at a depth of 0-7 cm in m3/m3. The soil moisture data is calculated from the ECMWF interim reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryJson[\"layers\"].extend([\n",
    "    {   \n",
    "        \"alias\" : \"TMax{y}\".format(y = y),\n",
    "        \"id\": '92',\n",
    "        \"output\": True,\n",
    "        \"temporal\" : {\"intervals\" : [{\n",
    "            \"start\" : datetime(y, 1, 1).strftime(iso8601), \"end\" : datetime(y+1, 1, 1).strftime(iso8601)\n",
    "        }]},\n",
    "        \"aggregation\" : \"Max\",\n",
    "        \"type\": \"raster\"\n",
    "    }\n",
    "    for y in range(yearStart, yearEnd)\n",
    "])\n",
    "queryJson[\"layers\"].extend([\n",
    "    {   \n",
    "        \"alias\" : \"SW{y}\".format(y = y),\n",
    "        \"id\": '49069',\n",
    "        \"output\": True,\n",
    "        \"temporal\" : {\"intervals\" : [{\n",
    "            \"start\" : datetime(y, 1, 1).strftime(iso8601), \"end\" : datetime(y+1, 1, 1).strftime(iso8601)\n",
    "        }]},\n",
    "        \"aggregation\" : \"Sum\",\n",
    "        \"type\": \"raster\"\n",
    "    }\n",
    "    for y in range(yearStart, yearEnd)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = paw.PAIRSQuery(queryJson)\n",
    "query.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Spatial aggregation is not the fastest process. This might take an hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.poll_till_finished()\n",
    "query.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.create_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the data is aggregated by state, PAIRS essentially returns a number of data frames in the form of csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our analysis is some simple data munging. We take the various data frames returned by PAIRS and wrapped in `query.data` and extract the features we are interested in. For the burned area data this is the area, which can be found in the `count()[unit: km^2]` statistic. For the maximum temperature we choose the spatial maximum, for the soil water content the spatial mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmaxData = list()\n",
    "burnData = list()\n",
    "swData = list()\n",
    "for k in query.data.keys():\n",
    "    swYear = re.match('Daily global weather\\-Daily SWVL 1 History\\[SW([0-9]{4})\\]\\-Sum', k)\n",
    "    tmaxYear = re.match('Daily US weather \\(PRISM\\)\\-Daily maximum temperature\\[TMax([0-9]{4})\\]\\-Max', k)\n",
    "    burnedYear = re.match('Burned area \\(MODIS\\)\\-Burned area\\[B([0-9]{4})\\]\\-Max', k)\n",
    "    \n",
    "    if tmaxYear:\n",
    "        query.data[k]['Year'] = int(tmaxYear.group(1))\n",
    "        tmaxData.append(query.data[k])\n",
    "    elif burnedYear:\n",
    "        query.data[k]['Year'] = int(burnedYear.group(1))\n",
    "        burnData.append(query.data[k])\n",
    "    elif swYear:\n",
    "        query.data[k]['Year'] = int(swYear.group(1))\n",
    "        swData.append(query.data[k])\n",
    "    else:\n",
    "        raise Exception('No match for key {}.'.format(k))\n",
    "tmaxData = pd.concat(tmaxData, 0).reset_index(drop = True)\n",
    "burnData = pd.concat(burnData, 0).reset_index(drop = True)\n",
    "swData = pd.concat(swData, 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we will analyze the data by state. To do so, we load information on the various AOIs from PAIRS. Strictly speaking we are not only aggregating by state since we treat the District of Columbia separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoiDetails = [requests.get('https://pairs.res.ibm.com/ws/queryaois/aoi/{aoi}'.format(aoi = aoi), auth = query.auth).json() for aoi in range(121, 172)]\n",
    "aoiDetails = pd.DataFrame(aoiDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTheData = pd.merge(\n",
    "    burnData[['PAIRS polygon ID', 'Year', 'count()[unit: km^2]']],\n",
    "    tmaxData[['PAIRS polygon ID', 'Year', 'max()']],\n",
    "    on = ['PAIRS polygon ID', 'Year'], how = 'outer'\n",
    ").rename(columns = {'count()[unit: km^2]' : 'BurnedArea', 'max()' : 'TMax'})\n",
    "allTheData['BurnedArea'].fillna(0, inplace = True)\n",
    "allTheData = pd.merge(allTheData, swData[['PAIRS polygon ID', 'Year', 'mean()']], on = ['PAIRS polygon ID', 'Year'], how = 'outer').rename(columns = {'mean()' : 'SoilWater'})\n",
    "allTheData = pd.merge(allTheData, aoiDetails[['id', 'name']], left_on = 'PAIRS polygon ID', right_on = 'id', how = 'inner').drop(['PAIRS polygon ID', 'id'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conterminous USA\n",
    "\n",
    "To start, we take a look at the conterminous USA. (If we wanted to analyze Hawaii or Alaska, we could not use the PRISM data. An obvious replacement would be datasets 190 or 157, the _Global weather (ERA5)_ and _Current and historical weather (IBM TWC)_ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conusData = allTheData.groupby('Year').aggregate({'BurnedArea' : np.sum, 'TMax' : np.mean, 'SoilWater' : np.mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conusData.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having take a look at correlations, we fit 3 models -- one for each combination of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conusModel = linear_model.LinearRegression()\n",
    "conusModel.fit(conusData[['TMax']] - conusData[['TMax']].mean(), conusData['BurnedArea'])\n",
    "conusModel.score(conusData[['TMax']] - conusData[['TMax']].mean(), conusData['BurnedArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conusModel2 = linear_model.LinearRegression()\n",
    "conusModel2.fit(conusData[['TMax', 'SoilWater']] - conusData[['TMax', 'SoilWater']].mean(), conusData['BurnedArea'])\n",
    "conusModel2.score(conusData[['TMax', 'SoilWater']] - conusData[['TMax', 'SoilWater']].mean(), conusData['BurnedArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conusModel3 = linear_model.LinearRegression()\n",
    "conusModel3.fit(conusData[['SoilWater']] - conusData[['SoilWater']].mean(), conusData['BurnedArea'])\n",
    "conusModel3.score(conusData[['SoilWater']] - conusData[['SoilWater']].mean(), conusData['BurnedArea'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one might expect the model containing both maximum temperature and soil water is best in terms of $R^2$ score. However, the improvement is marginal. For simplicity we continue our analysis focussing on a simple linear dependence between burned area and maximum temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(\n",
    "    conusData['TMax'].min() - conusData['TMax'].mean(),\n",
    "    conusData['TMax'].max() - conusData['TMax'].mean(), 100\n",
    ").reshape(-1, 1)\n",
    "y = conusModel.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(conusData['TMax'] - conusData['TMax'].mean(), conusData['BurnedArea'])\n",
    "plt.plot(X, y, color = 'red')\n",
    "plt.xlabel('Maximum temperature (deviation from mean) [K]')\n",
    "plt.ylabel('Burned area [Km2]')\n",
    "plt.title('Conterminous USA - Burned area vs. annual maximum temperature')\n",
    "plt.savefig('linearDependence.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we calculate some statistical scores. Specifically, we calculate\n",
    "\n",
    "$\n",
    "\\hat{\\beta} = (X^T X)^{-1} X^T y \\\\\n",
    "\\hat{y} = X \\hat{\\beta} \\\\\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N-p-1} \\sum (y - \\hat{y})^2 \\\\\n",
    "Var(\\hat{\\beta}) = (X^T X)^{-1} \\hat{\\sigma}^2 \\\\\n",
    "v_j = (X^T X)^{-1}_{jj} \\\\\n",
    "z_j = \\frac{\\hat{\\beta}_j}{\\hat{\\sigma} \\sqrt{v_j}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (conusData[['TMax']] - conusData[['TMax']].mean())\n",
    "X['1'] = 1\n",
    "X = X[['1', 'TMax']].values\n",
    "y = conusData['BurnedArea'].values\n",
    "\n",
    "betaHat = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.transpose(), X)), X.transpose()), y)\n",
    "yHat = np.matmul(X, betaHat)\n",
    "sigmaHat2 = np.power(y - yHat, 2).sum() / (len(y) - 1 - 1)\n",
    "VarBeta = np.linalg.inv(np.matmul(X.transpose(), X)) * sigmaHat2\n",
    "v = np.array([np.linalg.inv(np.matmul(X.transpose(), X))[i, i] for i in range(2)])\n",
    "zScore = betaHat / (np.sqrt(sigmaHat2 * v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-state analysis\n",
    "\n",
    "To proceed, we would like to evaluate and compare different states. This poses a problem however: How do we best compare areas affected by wildfires across states with vastly different spatial coverage? An additional $1 Km^2$ in Rhode Island is a very different situation to an additional $1 Km^2$ in Alaska. One possibility is to normalize the burned area we are considering before by the _burnable area_; i.e. the fraction of a state's area that is covered by forests, shrubland or other vegetation that is susceptible to wildfires. To do so, we will query layer `48522`, which contains land use data from the US Department of Agriculture. The original data has 30 m resolution. `48522` is a convenience product that contains the same information coarse-grained to 250 m.\n",
    "\n",
    "Note: One can certainly argue whether our methodology is correct. From an ecological perspective, every square kilometer of burned area is problematic, no matter where. Having said that, note that the mathematics of our linear fits will not be affected by a simple rescaling of the dependent variable.\n",
    "\n",
    "### Normalizing the area\n",
    "\n",
    "In either case, we proceed by grabbing the data from layer `48522`. In principle one could do this for each year or aggregate over multiple years. In what follows we simply use 2016 as a representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnableAreaQueryJson = {\n",
    "    \"layers\": [\n",
    "        {   \n",
    "            \"alias\" : \"crop2016\",\n",
    "            \"id\": '48522',\n",
    "            \"output\": False,\n",
    "            \"temporal\" : {\"intervals\" : [{\n",
    "                \"snapshot\" : datetime(2016, 1, 1).strftime(iso8601)\n",
    "            }]},\n",
    "            \"type\": \"raster\"\n",
    "        },\n",
    "        {   \n",
    "            \"alias\" : \"burnable2016\",\n",
    "            \"output\": True,\n",
    "            \"expression\" : \"0 + (\" + \" || \".join([\"($crop2016 == {})\".format(crop) for crop in burnableLand]) + \")\"\n",
    "        }\n",
    "    ],\n",
    "    \"name\": \"BurnedAreas\",\n",
    "    \"spatial\" : {\n",
    "        \"type\" : \"poly\", \"aoi\" : \"24\",\n",
    "        \"aggregation\" : {\"aoi\" : list(range(121, 172))}\n",
    "    },\n",
    "    \"temporal\": {\"intervals\": [{\"start\" : datetime(2019, 1, 1).strftime(iso8601), \"end\" : datetime(2020, 1, 1).strftime(iso8601)}]},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnableAreaQuery = paw.PAIRSQuery(burnableAreaQueryJson)\n",
    "burnableAreaQuery.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnableAreaQuery.poll_till_finished()\n",
    "burnableAreaQuery.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnableAreaQuery.create_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the query, we join the data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnableArea = pd.merge(\n",
    "    burnableAreaQuery.data['Expression-burnable2016[burnable2016]-Exp'],\n",
    "    aoiDetails[['id', 'name']], left_on = 'PAIRS polygon ID', right_on = 'id', how = 'inner'\n",
    ").drop(['PAIRS polygon ID', 'id', 'min()', 'max()', 'mean()', '2nd moment'], 1).rename(columns = {'count()[unit: km^2]' : 'BurnableArea'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and calculate the aforementioned fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenMoreData = pd.merge(allTheData, burnableArea, on = 'name', how = 'outer')\n",
    "evenMoreData['BurnedFraction'] = evenMoreData['BurnedArea'] / evenMoreData['BurnableArea'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in the position to fit a linear model for each state. As we are ignoring Alaska and Hawaii and are treating D.C. as a state, we have exactly 49 states which we can nicely arrange in a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(7, 7, figsize = (25, 25), sharex = True, sharey = True)\n",
    "betaHats, zScores, varBetaHats = dict(), dict(), dict()\n",
    "\n",
    "for i, state in evenMoreData.groupby('name').aggregate({'BurnedArea' : np.sum}).sort_values('BurnedArea', ascending = False).index.to_series().reset_index(drop = True).iteritems():\n",
    "    stateData = evenMoreData[evenMoreData['name'] == state]\n",
    "    \n",
    "    # We fit a model\n",
    "    stateModel = linear_model.LinearRegression()\n",
    "    stateModel.fit(stateData[['TMax']] - stateData[['TMax']].mean(), stateData['BurnedFraction'])\n",
    "    \n",
    "    # Evaluating the model\n",
    "    X = (stateData[['TMax']] - stateData[['TMax']].mean())\n",
    "    X['1'] = 1\n",
    "    X = X[['1', 'TMax']].values\n",
    "    y = stateData['BurnedFraction'].values\n",
    "\n",
    "    betaHat = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.transpose(), X)), X.transpose()), y)\n",
    "    yHat = np.matmul(X, betaHat)\n",
    "    sigmaHat2 = np.power(y - yHat, 2).sum() / (len(y) - 1 - 1)\n",
    "    varBetaHat = np.linalg.inv(np.matmul(X.transpose(), X)) * sigmaHat2\n",
    "    v = np.array([np.linalg.inv(np.matmul(X.transpose(), X))[i, i] for i in range(2)])\n",
    "    zScore = betaHat / (np.sqrt(sigmaHat2 * v))\n",
    "    \n",
    "    betaHats[state] = betaHat[1]\n",
    "    zScores[state] = zScore[1]\n",
    "    varBetaHats[state] = varBetaHat[1, 1]\n",
    "    \n",
    "    # Plotting\n",
    "    X = np.linspace(-3.5, 5.0, 100).reshape(-1, 1)\n",
    "    y = stateModel.predict(X)\n",
    "    \n",
    "    row = i//7\n",
    "    column = i % 7\n",
    "    \n",
    "    ax[row, column].scatter(stateData['TMax'] - stateData['TMax'].mean(), stateData['BurnedFraction'])\n",
    "    ax[row, column].plot(X, y, color = 'red')\n",
    "    ax[row, column].set_title(state)\n",
    "    \n",
    "for a in ax.flat:\n",
    "    a.set(xlabel='ΔTemperature [K]', ylabel='Area [%]')\n",
    "    a.label_outer()\n",
    "plt.savefig('linarDependenceByState.png', bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, we pick the states with significant z-scores. Interestingly, this does not include California, which shows us how noisy this data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSummary = pd.merge(pd.DataFrame({'z' : zScores, 'beta' : betaHats, 'Var(beta)' : varBetaHats}).sort_values('z', ascending = False), evenMoreData.groupby('name').aggregate({'BurnedArea' : np.sum, 'BurnedFraction' : np.mean}), left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreSummary[scoreSummary['z'].abs() > 2.0].sort_values('BurnedArea', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "Finally, we can visualize our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import shape, multipolygon\n",
    "import geojson, geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = [(aoi, shape(geojson.loads(requests.get('https://pairs.res.ibm.com/ws/queryaois/geojson/{aoi}'.format(aoi = aoi), auth = query.auth).json()))) for aoi in range(121, 172)]\n",
    "polygonsDF = pd.DataFrame(polygons, columns = ['id', 'Geometry'])\n",
    "polygonsDF = pd.merge(polygonsDF, aoiDetails[['id', 'name']]).drop(['id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredPolygons = geopandas.GeoDataFrame(pd.merge(scoreSummary, polygonsDF, left_index = True, right_on = 'name'), geometry = 'Geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize = (24, 8), sharex = True, sharey = True)\n",
    "\n",
    "scoredPolygons.plot(column = 'beta', cmap = 'RdYlGn_r', ax = ax[0][0], legend = True, vmax = scoredPolygons['beta'].abs().max(), vmin = -scoredPolygons['beta'].abs().max())\n",
    "ax[0][0].set_ylabel('Latitude')\n",
    "ax[0][0].set_title('beta - burned fraction vs. temperature [% area / K]')\n",
    "\n",
    "scoredPolygons.dropna().plot(column = 'BurnedFraction', cmap = 'Reds', ax = ax[0][1], legend = True)\n",
    "ax[0][1].set_title('Burned area fraction')\n",
    "\n",
    "scoredPolygons.plot(column = 'Var(beta)', cmap = 'Blues_r', ax = ax[1][0], legend = True)\n",
    "ax[1][0].set_ylabel('Latitude')\n",
    "ax[1][0].set_xlabel('Longitude')\n",
    "ax[1][0].set_title('Var(beta)')\n",
    "\n",
    "scoredPolygons.dropna().plot(column = 'z', cmap = 'coolwarm', ax = ax[1][1], legend = True, vmin = -2.2, vmax = 2.2)\n",
    "ax[1][1].set_xlabel('Longitude')\n",
    "ax[1][1].set_title('z-scores')\n",
    "\n",
    "plt.savefig('maps.png', dpi = 60, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123.4px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
